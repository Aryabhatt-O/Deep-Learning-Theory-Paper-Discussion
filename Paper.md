<!-- Simple islolation -->
   - [Comparative Generalization Bounds for Deep Neural Networks](https://openreview.net/forumid=162TqkUNPO&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DTMLR%2FAuthors%23your-submissions)
      - <details><summary>Amartya's Notes</summary>
        Recent research suggests that deep neural networks are able to generalize well to new data. This paper looks at how corrupted labels affect the extent of intermediate layer NCC separability. Authors have proposed a novel generalization bound that estimates the likelihood that the effective depth of a trained neural network is strictly smaller than the minimal depth required to achieve NCC separability with partially corrupted labels. They have introduced the concept of “effective depth’ in neural networks, which refers to the lowest layer at which the features are nearest class-center separable $$a_1$$
        </details>  
